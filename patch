diff -Naur linux-source-3.5.0/block/blk-account.c /remote/mnt/raid/workspace/linux-source-3.5.0/block/blk-account.c
--- linux-source-3.5.0/block/blk-account.c	1970-01-01 02:00:00.000000000 +0200
+++ /remote/mnt/raid/workspace/linux-source-3.5.0/block/blk-account.c	2012-12-15 16:47:56.000694338 +0200
@@ -0,0 +1,567 @@
+/* bp_account_record() and friends */
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/mm.h>
+#include <linux/swap.h>
+#include <linux/poll.h>
+#include <linux/pagemap.h>
+#include <linux/hugetlb.h>
+/* NADAV bp */
+#include <linux/kthread.h>
+#include <linux/freezer.h>
+#include <linux/proc_fs.h>
+#include <linux/ratelimit.h>
+
+#include <linux/blkdev.h>
+#include <linux/fs.h>
+#include <linux/buffer_head.h>
+
+struct bp_request_line {
+	unsigned long block;		/* 8  (8) */
+	int size;			/* 4  (12) */
+	unsigned int devid;		/* 4  (16) */
+};
+
+struct bp_account_line {
+	unsigned long jiffies;		/* 8  (8) */
+	unsigned long i_ino;		/* 8  (16) */
+	unsigned long block;		/* 8  (24) */
+	unsigned int devid;		/* 4  (28) */
+	unsigned int pgdevid;		/* 4  (32) */
+	int pgindex;			/* 4  (36) */
+	int size;			/* 4  (40) */
+	int pid;			/* 4  (44) */
+	int tgid;			/* 4  (48) */
+	int reason;			/* 4  (52) */
+	char comm[TASK_COMM_LEN];	/* 16 (68) */
+	char devname[BDEVNAME_SIZE];	/* 32 (100) */
+} __packed;
+
+static DEFINE_PER_CPU(int, bp_exiting);
+
+/* Exclusive producer (CPU #n), Exclusive reader (by mutex). All synchronization is lazy (except one barrier in writer) */
+struct bp_buffer {
+	int rptr ____cacheline_aligned;
+	int wptr ____cacheline_aligned;
+};
+static DEFINE_PER_CPU(struct bp_buffer, bp_account_buffer);
+static DEFINE_PER_CPU(struct bp_account_line *, bp_line_buffer);
+
+#define BP_SHIFT	(15)
+#define BP_SIZE		(1 << BP_SHIFT)
+#define BP_MASK		(BP_SIZE-1)
+#define BP_INC(x)	(((x)+1)&BP_MASK)
+
+/* After MIN_FILL lines, we wake the thread up immediately */
+#define MIN_FILL	(BP_SIZE >> 1)
+
+static DEFINE_PER_CPU(struct mutex, bp_mutex);
+
+static DEFINE_PER_CPU(wait_queue_head_t, bp_user_wait);
+static DEFINE_PER_CPU(struct proc_dir_entry *, bp_proc_kbpd);
+
+struct bp_statistics_thread {
+	wait_queue_head_t wait;
+	struct task_struct *tsk;
+	struct completion start_done;
+};
+static struct bp_statistics_thread bp_statistics_thread = {
+	.wait = __WAIT_QUEUE_HEAD_INITIALIZER(bp_statistics_thread.wait),
+	.tsk = NULL,
+	.start_done = {
+		.done = 0,
+		.wait = __WAIT_QUEUE_HEAD_INITIALIZER(bp_statistics_thread.start_done.wait),
+	},
+};
+
+static unsigned long bp_last_print = 0;
+#define bp_printk(fmt, args...)		do { if (printk_timed_ratelimit(&bp_last_print, HZ >> 2)) printk(fmt, ##args); } while (0)
+#ifndef STRINGIFYFY
+#define STRINGIFYFY(i) #i
+#endif
+#ifndef STRINGIFY
+#define STRINGIFY(i) STRINGIFYFY(i)
+#endif
+#define bp_dprintk(fmt, args...)	do { 						\
+		static DEFINE_RATELIMIT_STATE(____ratelimit_state, 20 * HZ, 5);		\
+		if (___ratelimit(&____ratelimit_state, __FILE__ STRINGIFY(__LINE__)))	\
+			printk(fmt, ##args);						\
+} while (0)
+#define dbp_printk	bp_dprintk
+//#define dbp_printk(fmt, args...)	do { } while (0)
+
+static inline int __bp_fill(int wptr, int rptr) {
+	return ((wptr + BP_SIZE - rptr) & BP_MASK);
+}
+
+static inline int _bp_fill(struct bp_buffer *bp_buffer) {
+	return __bp_fill(bp_buffer->wptr, bp_buffer->rptr);
+}
+
+static void bp_account_record(struct block_device *bdev, unsigned int pgdevid, unsigned long i_ino, int index, unsigned long block, int size, int reason)
+{
+	struct bp_buffer *bp_buffer;
+	int wptr;
+	struct bp_account_line *buf;
+
+	BUILD_BUG_ON(sizeof(struct bp_account_line) != 100);
+
+	BUG_ON(preemptible());
+
+	if (unlikely(__get_cpu_var(bp_exiting) == 1))
+		return;
+	bp_buffer = this_cpu_ptr(&bp_account_buffer);
+	wptr = bp_buffer->wptr;
+	if (unlikely(BP_INC(wptr) == bp_buffer->rptr))
+		return;
+	buf = __get_cpu_var(bp_line_buffer);
+	buf[wptr].jiffies = jiffies;
+	buf[wptr].i_ino = i_ino;
+	buf[wptr].block = block;
+	buf[wptr].devid = (bdev)?bdev->bd_dev:0;
+	buf[wptr].pgdevid = pgdevid;
+	buf[wptr].pgindex = index;
+	buf[wptr].size = size;
+	buf[wptr].reason = reason;
+	if (current) {
+		buf[wptr].pid = task_pid_nr(current);
+		buf[wptr].tgid = task_tgid_nr(current);
+		strncpy(buf[wptr].comm, current->comm, TASK_COMM_LEN);
+	} else {
+		buf[wptr].pid = 0;
+		buf[wptr].tgid = 0;
+		strncpy(buf[wptr].comm, "NoTask", TASK_COMM_LEN);
+	}
+	if (bdev) {
+		memset(buf[wptr].devname, 0, BDEVNAME_SIZE);
+		bdevname(bdev, buf[wptr].devname);
+	} else
+		strncpy(buf[wptr].devname, "NODEVNAME", BDEVNAME_SIZE);
+	/* The data must be visible before incrementing the write pointer */
+	smp_wmb();
+	bp_buffer->wptr = BP_INC(wptr);
+	if (waitqueue_active(this_cpu_ptr(&bp_user_wait)))
+		wake_up_interruptible(this_cpu_ptr(&bp_user_wait));
+}
+
+#if 0
+static void bp_account_record(struct block_device *bdev, unsigned long block, int size, int reason)
+{
+	_bp_account_record(bdev, 0, 0, 0, block, size, reason);
+}
+static void bp_account_page_block(struct block_device *bdev, unsigned long block, int size, unsigned int devid, unsigned long ino, int index, int reason)
+{
+	_bp_account_record(bdev, devid, ino, index, block, size, reason);
+}
+static void bp_account_page(unsigned int devid, unsigned long ino, int index, int reason)
+{
+	_bp_account_record(NULL, devid, ino, index, 0, 0, reason);
+}
+#endif
+
+static inline int _bp_exiting(void) {
+	int ret;
+	preempt_disable();
+	ret = __get_cpu_var(bp_exiting);
+	preempt_enable();
+	return ret;
+}
+
+static int bp_statistics_thread_worker(void *arg)
+{
+	unsigned long next_time = jiffies + (HZ * 20);
+
+	printk(KERN_DEBUG "Starting bp_statistics_global thread pid %d\n", task_pid_nr(current));
+
+	complete(&bp_statistics_thread.start_done);
+
+	set_current_state(TASK_INTERRUPTIBLE);
+
+	set_freezable();
+
+	while (!kthread_should_stop()) {
+		int cpu;
+		int datums = 0;
+
+		preempt_disable();
+		if (__get_cpu_var(bp_exiting) == 1) {
+			preempt_enable();
+			printk(KERN_ERR "Exiting - break statistics_thread\n");
+			break;
+		}
+		preempt_enable();
+
+		if (jiffies < next_time) {
+			wait_event_interruptible_timeout(bp_statistics_thread.wait,
+							 kthread_should_stop(),
+							 next_time - jiffies);
+			try_to_freeze();
+			continue;
+		}
+
+		__set_current_state(TASK_RUNNING);
+
+		next_time = jiffies + (HZ * 20);
+
+		for_each_online_cpu(cpu) {
+			struct bp_buffer *bp_buffer = &per_cpu(bp_account_buffer, cpu);
+			datums += _bp_fill(bp_buffer);
+		}
+		printk(KERN_INFO "BP %lu bytes waiting for user in %d buffers\n", datums * sizeof(struct bp_account_line), datums);
+
+		if (need_resched())
+			cond_resched();
+
+		try_to_freeze();
+
+		set_current_state(TASK_INTERRUPTIBLE);
+	}
+
+	printk(KERN_ERR "Stopping %s\n", bp_statistics_thread.tsk->comm);
+
+	/* Wait for kthread_stop */
+	while (!kthread_should_stop()) {
+		set_current_state(TASK_INTERRUPTIBLE);
+		schedule();
+	}
+	__set_current_state(TASK_RUNNING);
+
+	return 0;
+}
+
+static DEFINE_PER_CPU(int, bp_file_busy);
+
+static int bp_global_open(struct inode *inode, struct file *file)
+{
+	int cpu = (long)(PDE(inode)->data);
+	if (try_module_get(THIS_MODULE)) {
+		if (mutex_trylock(&per_cpu(bp_mutex, cpu))) {
+			if (per_cpu(bp_file_busy, cpu)) {
+				bp_dprintk(KERN_ERR "open returns busy\n");
+				mutex_unlock(&per_cpu(bp_mutex, cpu));
+				return -EBUSY;
+			}
+			per_cpu(bp_file_busy, cpu) = 1;
+			mutex_unlock(&per_cpu(bp_mutex, cpu));
+			file->private_data = (void *)((long)cpu);
+			return 0;
+		}
+		module_put(THIS_MODULE);
+		bp_dprintk(KERN_ERR "open returns busy (mutex is contended)\n");
+		return -EBUSY;
+	} else {
+		bp_dprintk(KERN_ERR "try_module_get failed - open returns error\n");
+		return -EIO;
+	}
+}
+
+static int bp_global_close(struct inode* inode, struct file* filp)
+{
+	int cpu = (long)(PDE(inode)->data);
+	BUG_ON(filp->private_data != (void *)((long)cpu));
+	BUG_ON(!per_cpu(bp_file_busy, cpu));
+	per_cpu(bp_file_busy, cpu) = 0;
+	module_put(THIS_MODULE);
+	return 0;
+}
+
+static inline int bp_empty(int cpu)
+{
+	struct bp_buffer *bp_buffer = &per_cpu(bp_account_buffer, cpu);
+	return (bp_buffer->rptr == bp_buffer->wptr);
+}
+
+static unsigned int bp_global_poll(struct file *filp, poll_table *wait)
+{
+	int cpu = (long)(filp->private_data);
+	poll_wait(filp, &per_cpu(bp_user_wait, cpu), wait);
+
+	if (unlikely(per_cpu(bp_exiting, cpu) == 1)) {
+		return POLLIN | POLLRDNORM | POLLERR;
+	}
+	if (bp_empty(cpu))
+		return 0;
+	return POLLIN | POLLRDNORM;
+}
+
+static inline size_t iov_total(const struct iovec *iv, unsigned long count)
+{
+	unsigned long i;
+	size_t len;
+
+	for (i = 0, len = 0; i < count; i++)
+		len += iv[i].iov_len;
+
+	return len;
+}
+
+static ssize_t bp_global_write(struct file *filp, const char __user *buf, size_t len, loff_t *ppos)
+{
+//	int cpu = (long)(filp->private_data);
+	struct block_device *bdev;
+	struct bp_request_line bp;
+	struct buffer_head *bh;
+	int read_size;
+	int read = 0;
+
+	if (unlikely(len == 0))
+		return 0;
+
+	dbp_printk(KERN_INFO "Write\n");
+
+	if (len != sizeof(struct bp_request_line)) {
+		bp_printk(KERN_ERR "Bad write size\n");
+		return -EINVAL;
+	}
+	if (copy_from_user(&bp, (void *)buf, len)) {
+		bp_printk(KERN_ERR "Efault\n");
+		return -EFAULT;
+	}
+
+	dbp_printk(KERN_INFO "Looking up device %x\n", bp.devid);
+	bdev = bdget(bp.devid);
+	if (IS_ERR(bdev)) {
+		bp_dprintk(KERN_ERR "Can't get device %x err %ld\n", bp.devid, PTR_ERR(bdev));
+		return -ENODEV;
+	}
+	if (bdev == NULL) {
+		bp_dprintk(KERN_ERR "Can't get device %x\n", bp.devid);
+		return -ENODEV;
+	}
+	read_size = bdev_logical_block_size(bdev);
+	if (read_size != 512) {
+		bp_dprintk(KERN_ERR "Logical read size is odd (%d)\n", read_size);
+		bdput(bdev);
+		return -EINVAL;
+	}
+	/* The pagecache uses (1<<PAGE_CACHE_SHIFT) sized blocks (one page) */
+	if (bdev->bd_inode->i_blkbits != PAGE_CACHE_SHIFT) {
+		bp_dprintk(KERN_ERR "Device block size (%d) different from page cache size (%d)\n", 1 << bdev->bd_inode->i_blkbits, 1 << PAGE_CACHE_SHIFT);
+		bdput(bdev);
+		return -EINVAL;
+	}
+	bp.size = (bp.size + ((1 << PAGE_CACHE_SHIFT)-1)) >> PAGE_CACHE_SHIFT; /* Round up */
+	dbp_printk(KERN_INFO "Reading %lu bytes\n", bp.size * PAGE_CACHE_SIZE);
+	while (read < bp.size) {
+		bh = __bread(bdev, bp.block, PAGE_CACHE_SIZE);
+		if (bh) {
+			lock_buffer(bh);
+			if (!buffer_uptodate(bh)) {
+				bp_dprintk(KERN_INFO "Buffer %d not up to date\n", (int)bp.block);
+			}
+			unlock_buffer(bh);
+			brelse(bh);
+			bp.block++;
+			read++;
+		} else {
+			bp_dprintk(KERN_ERR "Failed to get buffer head for %d\n", (int)bp.block);
+			bdput(bdev);
+			return -EAGAIN;
+		}
+		if (need_resched())
+			cond_resched();
+	}
+	bdput(bdev);
+	return len;
+}
+
+static ssize_t bp_global_read(struct file *filp, char __user *buf, size_t len, loff_t *ppos)
+{
+	int cpu = (long)(filp->private_data);
+	char __user *out = buf;
+	ssize_t read = 0;
+	int res = 0;
+	struct bp_buffer *bp_buffer = &per_cpu(bp_account_buffer, cpu);
+	int rptr = bp_buffer->rptr; /* We're exclusive writers by mutex lock */
+
+	if (unlikely(len == 0))
+		return 0;
+	if (unlikely(len < sizeof(struct bp_account_line)))
+		return -ETOOSMALL;
+	len -= (len % sizeof(struct bp_account_line));
+
+	while (len) {
+		int to_copy;
+		to_copy = __bp_fill(bp_buffer->wptr, rptr);
+		if (to_copy == 0) {
+			if (filp->f_flags & O_NONBLOCK) {
+				res = -EAGAIN;
+				break;
+			}
+			if (wait_event_interruptible(per_cpu(bp_user_wait, cpu), !bp_empty(cpu))) {
+				res = -EINTR;
+				break;
+			}
+			to_copy = __bp_fill(bp_buffer->wptr, rptr);
+		}
+		/* Overflow */
+		if (((rptr + to_copy) & BP_MASK) != (rptr + to_copy))
+			to_copy = BP_SIZE-rptr;
+		if ((to_copy * sizeof(struct bp_account_line)) > len)
+			to_copy = len / sizeof(struct bp_account_line);
+		BUG_ON( to_copy == 0 );
+		if (copy_to_user(out, &(per_cpu(bp_line_buffer, cpu)[rptr]), to_copy * sizeof(struct bp_account_line))) {
+			res = -EFAULT;
+			break;
+		}
+		/* If overflow before, rptr will now be 0 */
+		rptr = (rptr + to_copy) & BP_MASK;
+		bp_buffer->rptr = rptr;
+		read += to_copy * sizeof(struct bp_account_line);
+		out += to_copy * sizeof(struct bp_account_line);
+		len -= to_copy * sizeof(struct bp_account_line);
+	}
+	if (read)
+		return read;
+	return res;
+}
+
+static struct proc_dir_entry *bp_proc_dir;
+static const struct file_operations bp_global_fops = {
+	.owner    = THIS_MODULE,
+	.llseek   = no_llseek,
+	.open     = bp_global_open,
+	.release  = bp_global_close,
+	.read     = bp_global_read,
+	.write    = bp_global_write,
+	.poll     = bp_global_poll,
+};
+
+static int __init bp_create_global_statistics_thread(void)
+{
+	struct task_struct *p;
+
+	p = kthread_create_on_node(bp_statistics_thread_worker,
+				   &bp_statistics_thread,
+				   -1,
+				   "kbp_statd");
+	if (IS_ERR(p)) {
+		printk(KERN_ERR "kernel_thread() failed for global statistics thread\n");
+		return PTR_ERR(p);
+	}
+	bp_statistics_thread.tsk = p;
+
+	/* Low priority */
+	set_user_nice(p, 19);
+
+	wake_up_process(p);
+	wait_for_completion(&bp_statistics_thread.start_done);
+
+	return 0;
+}
+
+typedef void (bp_hook_fn) (struct block_device *bdev, unsigned int pgdevid, unsigned long i_ino, int index, unsigned long block, int size, int reason);
+extern int bp_hook_install(bp_hook_fn *fn);
+extern int bp_hook_uninstall(bp_hook_fn *fn);
+
+#define BP_PROC_DIR "bp"
+static void __exit bp_account_exit(void)
+{
+	int i;
+
+	for_each_possible_cpu(i) {
+		char proc_name[32];
+		per_cpu(bp_exiting, i) = 1;
+		/* wake_up contains barriers - the process will see the bp_exiting set above */
+		wake_up_interruptible(&per_cpu(bp_user_wait, i));
+		sprintf(proc_name, "kbpd%d", i);
+		remove_proc_entry(proc_name, bp_proc_dir);
+	}
+	remove_proc_entry(BP_PROC_DIR, NULL);
+
+	kthread_stop(bp_statistics_thread.tsk);
+
+	/* after bp_hook_uninstall, no new calls to bp_account_record will be made. It might still be called while running bp_hook_uninstall */
+	if (bp_hook_uninstall(bp_account_record)) {
+		printk(KERN_WARNING "BP: exit - hook already not installed\n");
+	}
+
+	/* Now we can release everything */
+	for_each_possible_cpu(i) {
+		kfree(per_cpu(bp_line_buffer, i));
+	}
+}
+
+static int __init bp_account_init(void)
+{
+	int i;
+	int err;
+
+	for_each_possible_cpu(i) {
+		mutex_init(&per_cpu(bp_mutex, i));
+		init_waitqueue_head(&per_cpu(bp_user_wait, i));
+		per_cpu(bp_file_busy, i) = 0;
+		per_cpu(bp_exiting, i) = 0;
+		per_cpu(bp_account_buffer, i).rptr = 0;
+		per_cpu(bp_account_buffer, i).wptr = 0;
+		per_cpu(bp_line_buffer, i) = kmalloc(sizeof(struct bp_account_line) * BP_SIZE, GFP_KERNEL);
+		if (per_cpu(bp_line_buffer, i) == NULL) {
+			int j;
+			for_each_possible_cpu(j) {
+				if (j == i)
+					break;
+				kfree(per_cpu(bp_line_buffer, j));
+			}
+			return -ENOMEM;
+		}
+	}
+
+	bp_proc_dir = proc_mkdir(BP_PROC_DIR, NULL);
+	if (!bp_proc_dir) {
+		err = -EIO;
+		goto free_all_buffers;
+	}
+
+	for_each_possible_cpu(i) {
+		char proc_name[32];
+		sprintf(proc_name, "kbpd%d", i);
+		per_cpu(bp_proc_kbpd, i) = proc_create_data(proc_name, 0600, bp_proc_dir,
+				      &bp_global_fops, (void *)((long)i));
+		if (per_cpu(bp_proc_kbpd, i) == NULL) {
+			int j;
+			printk(KERN_ERR "cannot create kbpd procfs entry\n");
+			for_each_possible_cpu(j) {
+				char old_proc_name[32];
+				if (j == i)
+					break;
+				sprintf(old_proc_name, "kbpd%d", j);
+				remove_proc_entry(old_proc_name, bp_proc_dir);
+			}
+			err = -EIO;
+			goto remove_bp_proc;
+		}
+	}
+
+	err = bp_create_global_statistics_thread();
+	if (err) {
+		printk(KERN_WARNING "BP: Cannot create global statistics thread (%d)\n",
+			   err);
+		goto remove_kbpd_proc;
+	}
+
+	if (bp_hook_install(bp_account_record)) {
+		printk(KERN_WARNING "BP: Cannot install hook\n");
+		goto stop_global;
+	}
+	return 0;
+
+stop_global:
+	kthread_stop(bp_statistics_thread.tsk);
+remove_kbpd_proc:
+	for_each_possible_cpu(i) {
+		char proc_name[32];
+		sprintf(proc_name, "kbpd%d", i);
+		remove_proc_entry(proc_name, bp_proc_dir);
+	}
+remove_bp_proc:
+	remove_proc_entry(BP_PROC_DIR, NULL);
+free_all_buffers:
+	for_each_possible_cpu(i) {
+		kfree(per_cpu(bp_line_buffer, i));
+	}
+	return err;
+}
+
+module_init(bp_account_init);
+module_exit(bp_account_exit);
+MODULE_LICENSE("GPL");
diff -Naur linux-source-3.5.0/block/Kconfig /remote/mnt/raid/workspace/linux-source-3.5.0/block/Kconfig
--- linux-source-3.5.0/block/Kconfig	2012-07-21 23:58:29.000000000 +0300
+++ /remote/mnt/raid/workspace/linux-source-3.5.0/block/Kconfig	2012-12-08 21:51:13.000000000 +0200
@@ -23,6 +23,16 @@
 
 if BLOCK
 
+config BLOCK_ACCOUNT
+	tristate "Account block layer transactions"
+	help
+	  Enable block device accounting.
+
+	  This data is read by userspace daemon
+	  To compile this file system support as a module, choose M here: the
+	  module will be called blk_account.
+	  If unsure, say N.
+
 config LBDAF
 	bool "Support for large (2TB+) block devices and files"
 	depends on !64BIT
diff -Naur linux-source-3.5.0/fs/buffer.c /remote/mnt/raid/workspace/linux-source-3.5.0/fs/buffer.c
--- linux-source-3.5.0/fs/buffer.c	2012-11-13 19:49:36.000000000 +0200
+++ /remote/mnt/raid/workspace/linux-source-3.5.0/fs/buffer.c	2012-12-15 12:42:01.140552090 +0200
@@ -162,6 +162,128 @@
 }
 EXPORT_SYMBOL(end_buffer_write_sync);
 
+typedef void (bp_hook_fn) (struct block_device *bdev, unsigned int pgdevid, unsigned long i_ino, int index, unsigned long block, int size, int reason);
+static DEFINE_SPINLOCK(bp_account_lock);
+static int bp_hook_enabled = 0;
+bp_hook_fn __rcu *bp_hook = NULL;
+int bp_hook_install(bp_hook_fn *fn)
+{
+	int res = 1;
+	spin_lock(&bp_account_lock);
+	if (!bp_hook_enabled && fn) {
+		res = 0;
+		rcu_assign_pointer(bp_hook, fn);
+		bp_hook_enabled = 1;
+	}
+	spin_unlock(&bp_account_lock);
+	return res;
+}
+EXPORT_SYMBOL(bp_hook_install);
+
+int bp_hook_uninstall(bp_hook_fn *fn)
+{
+	int res;
+	spin_lock(&bp_account_lock);
+	res = !bp_hook_enabled;
+	bp_hook_enabled = 0;
+	rcu_assign_pointer(bp_hook, NULL);
+	spin_unlock(&bp_account_lock);
+	synchronize_rcu();
+	return res;
+}
+EXPORT_SYMBOL(bp_hook_uninstall);
+
+static inline void _bp_account_hook(struct block_device *bdev, unsigned int pgdevid, unsigned long i_ino, int index, unsigned long block, int size, int reason)
+{
+	bp_hook_fn *func;
+	rcu_read_lock();
+	func = rcu_dereference(bp_hook);
+	if (func)
+		(*func)(bdev, pgdevid, i_ino, index, block, size, reason);
+	rcu_read_unlock();
+}
+void bp_account_hook(struct buffer_head *bh, int reason)
+{
+	if (bh->b_bdev && (bh->b_state & (1 << BH_Mapped)))
+		_bp_account_hook(bh->b_bdev, 0, 0, 0, bh->b_blocknr, bh->b_size, reason);
+}
+EXPORT_SYMBOL(bp_account_hook);
+
+void bp_account_page_hook(struct page *page, int reason)
+{
+	struct address_space *mapping = page_mapping(page);
+	if (!mapping)
+		return;
+	if (PageSwapCache(page))
+		return;
+	if (PageSwapBacked(page))
+		return;
+	_bp_account_hook(NULL, mapping->host->i_sb->s_dev, mapping->host->i_ino, page->index, 0, 0, reason);
+}
+EXPORT_SYMBOL(bp_account_page_hook);
+
+void bp_account_page_buffer(struct page *page, struct buffer_head *bh, int reason)
+{
+	struct address_space *mapping = page_mapping(page);
+	if (!mapping)
+		return;
+	if (PageSwapCache(page))
+		return;
+	if (PageSwapBacked(page))
+		return;
+	if (bh->b_bdev && (bh->b_state & (1 << BH_Mapped)))
+		_bp_account_hook(bh->b_bdev, mapping->host->i_sb->s_dev, mapping->host->i_ino, page->index, bh->b_blocknr, bh->b_size, reason);
+}
+EXPORT_SYMBOL(bp_account_page_buffer);
+
+void bp_account_page_locked(struct page *page, int reason)
+{
+	struct address_space *mapping = page_mapping(page);
+	int mapped = 0;
+	if (!mapping)
+		return;
+	if (PageSwapCache(page))
+		return;
+	if (PageSwapBacked(page))
+		return;
+	if (page_has_buffers(page)) {
+		struct buffer_head *head = page_buffers(page);
+		struct buffer_head *bh = head;
+		do {
+			if (bh->b_bdev && (bh->b_state & (1 << BH_Mapped))) {
+				mapped = 1;
+				_bp_account_hook(bh->b_bdev, mapping->host->i_sb->s_dev, mapping->host->i_ino, page->index, bh->b_blocknr, bh->b_size, reason);
+			}
+			bh = bh->b_this_page;
+		} while (bh != head);
+	}
+	if (!mapped)
+		_bp_account_hook(NULL, mapping->host->i_sb->s_dev, mapping->host->i_ino, page->index, 0, 0, reason);
+}
+EXPORT_SYMBOL(bp_account_page_locked);
+
+#if 0
+void bp_account_page(struct page *page, int reason)
+{
+	struct address_space *mapping = page_mapping(page);
+	struct address_space *mapping2;
+	unsigned long flags;
+	if (unlikely(!mapping))
+		return;
+	spin_lock_irqsave(&mapping->tree_lock, flags);
+	mapping2 = page_mapping(page);
+	if (likely(mapping2)) { /* Race with truncate? */
+		BUG_ON(mapping2 != mapping);
+		/* We're not holding the private_lock that protects this, but we're safe:
+		   The page is held (possibly mapped to running processes)
+		   You can't change the buffer-mapping without zapping it */
+		bp_account_page_locked(page, reason);
+	}
+	spin_unlock_irqrestore(&mapping->tree_lock, flags);
+}
+EXPORT_SYMBOL(bp_account_page);
+#endif
+
 /*
  * Various filesystems appear to want __find_get_block to be non-blocking.
  * But it's the page lock which protects the buffers.  To get around this,
@@ -616,6 +738,7 @@
 	spin_lock_irq(&mapping->tree_lock);
 	if (page->mapping) {	/* Race with truncate? */
 		WARN_ON_ONCE(warn && !PageUptodate(page));
+		bp_account_page_locked(page, BP_ACCOUNT_DIRTY);
 		account_page_dirtied(page, mapping);
 		radix_tree_tag_set(&mapping->page_tree,
 				page_index(page), PAGECACHE_TAG_DIRTY);
@@ -1119,6 +1242,7 @@
 
 	if (!test_set_buffer_dirty(bh)) {
 		struct page *page = bh->b_page;
+		bp_account_page_buffer(page, bh, BP_ACCOUNT_DIRTY);
 		if (!TestSetPageDirty(page)) {
 			struct address_space *mapping = page_mapping(page);
 			if (mapping)
@@ -1308,8 +1432,13 @@
 		if (bh)
 			bh_lru_install(bh);
 	}
-	if (bh)
+	if (bh) {
 		touch_buffer(bh);
+		if (BP_IS_READ())
+			bp_account_hook(bh, BP_ACCOUNT_READ);
+		else
+			bp_account_hook(bh, BP_ACCOUNT_GET);
+	}
 	return bh;
 }
 EXPORT_SYMBOL(__find_get_block);
@@ -1505,6 +1634,8 @@
 	tail->b_this_page = head;
 
 	spin_lock(&page->mapping->private_lock);
+	if (!PageDirty(page))
+		bp_account_page_locked(page, BP_ACCOUNT_GET);
 	if (PageUptodate(page) || PageDirty(page)) {
 		bh = head;
 		do {
@@ -1512,6 +1643,10 @@
 				set_buffer_dirty(bh);
 			if (PageUptodate(page))
 				set_buffer_uptodate(bh);
+			if (buffer_dirty(bh))
+				bp_account_page_buffer(page, bh, BP_ACCOUNT_DIRTY);
+			else
+				bp_account_page_buffer(page, bh, BP_ACCOUNT_GET);
 			bh = bh->b_this_page;
 		} while (bh != head);
 	}
@@ -2393,8 +2528,10 @@
 	spin_lock(&page->mapping->private_lock);
 	bh = head;
 	do {
-		if (PageDirty(page))
+		if (PageDirty(page)) {
 			set_buffer_dirty(bh);
+			bp_account_page_buffer(page, bh, BP_ACCOUNT_DIRTY);
+		}
 		if (!bh->b_this_page)
 			bh->b_this_page = head;
 		bh = bh->b_this_page;
@@ -3087,6 +3224,7 @@
 	}
 
 	spin_lock(&mapping->private_lock);
+	bp_account_page_hook(page, BP_ACCOUNT_EVICTED);
 	ret = drop_buffers(page, &buffers_to_free);
 
 	/*
@@ -3112,6 +3250,7 @@
 
 		do {
 			struct buffer_head *next = bh->b_this_page;
+			bp_account_page_buffer(page, bh, BP_ACCOUNT_EVICTED);
 			free_buffer_head(bh);
 			bh = next;
 		} while (bh != buffers_to_free);
diff -Naur linux-source-3.5.0/fs/direct-io.c /remote/mnt/raid/workspace/linux-source-3.5.0/fs/direct-io.c
--- linux-source-3.5.0/fs/direct-io.c	2012-07-21 23:58:29.000000000 +0300
+++ /remote/mnt/raid/workspace/linux-source-3.5.0/fs/direct-io.c	2012-12-01 17:44:38.000000000 +0200
@@ -1063,6 +1063,8 @@
 	size_t bytes;
 	struct buffer_head map_bh = { 0, };
 
+	if (rw == READ)
+		BP_SET_READ();
 	if (rw & WRITE)
 		rw = WRITE_ODIRECT;
 
@@ -1271,6 +1273,8 @@
 		BUG_ON(retval != -EIOCBQUEUED);
 
 out:
+	if (rw == READ)
+		BP_UNSET_READ();
 	return retval;
 }
 
diff -Naur linux-source-3.5.0/fs/ext3/inode.c /remote/mnt/raid/workspace/linux-source-3.5.0/fs/ext3/inode.c
--- linux-source-3.5.0/fs/ext3/inode.c	2012-11-13 19:49:36.000000000 +0200
+++ /remote/mnt/raid/workspace/linux-source-3.5.0/fs/ext3/inode.c	2012-12-08 20:01:59.000000000 +0200
@@ -1036,6 +1036,10 @@
 	if (ret > 0) {
 		bh_result->b_size = (ret << inode->i_blkbits);
 		ret = 0;
+		if (BP_IS_READ())
+			bp_account_hook(bh_result, BP_ACCOUNT_READ);
+		else
+			bp_account_hook(bh_result, BP_ACCOUNT_GET);
 	}
 	if (started)
 		ext3_journal_stop(handle);
diff -Naur linux-source-3.5.0/fs/ext4/inode.c /remote/mnt/raid/workspace/linux-source-3.5.0/fs/ext4/inode.c
--- linux-source-3.5.0/fs/ext4/inode.c	2012-11-13 19:49:36.000000000 +0200
+++ /remote/mnt/raid/workspace/linux-source-3.5.0/fs/ext4/inode.c	2012-12-08 19:57:54.000000000 +0200
@@ -692,6 +692,10 @@
 		bh->b_state = (bh->b_state & ~EXT4_MAP_FLAGS) | map.m_flags;
 		bh->b_size = inode->i_sb->s_blocksize * map.m_len;
 		ret = 0;
+		if (BP_IS_READ())
+			bp_account_hook(bh, BP_ACCOUNT_READ);
+		else
+			bp_account_hook(bh, BP_ACCOUNT_GET);
 	}
 	if (started)
 		ext4_journal_stop(handle);
diff -Naur linux-source-3.5.0/fs/ioctl.c /remote/mnt/raid/workspace/linux-source-3.5.0/fs/ioctl.c
--- linux-source-3.5.0/fs/ioctl.c	2012-07-21 23:58:29.000000000 +0300
+++ /remote/mnt/raid/workspace/linux-source-3.5.0/fs/ioctl.c	2012-12-01 17:45:24.000000000 +0200
@@ -60,7 +60,9 @@
 	res = get_user(block, p);
 	if (res)
 		return res;
+	BP_SET_READ();
 	res = mapping->a_ops->bmap(mapping, block);
+	BP_UNSET_READ();
 	return put_user(res, p);
 }
 
@@ -407,7 +409,9 @@
 {
 	int ret;
 	mutex_lock(&inode->i_mutex);
+	BP_SET_READ();
 	ret = __generic_block_fiemap(inode, fieinfo, start, len, get_block);
+	BP_UNSET_READ();
 	mutex_unlock(&inode->i_mutex);
 	return ret;
 }
diff -Naur linux-source-3.5.0/fs/jbd/commit.c /remote/mnt/raid/workspace/linux-source-3.5.0/fs/jbd/commit.c
--- linux-source-3.5.0/fs/jbd/commit.c	2012-07-21 23:58:29.000000000 +0300
+++ /remote/mnt/raid/workspace/linux-source-3.5.0/fs/jbd/commit.c	2012-12-10 14:30:34.000000000 +0200
@@ -138,6 +138,7 @@
 
 	JBUFFER_TRACE(descriptor, "write commit block");
 	set_buffer_dirty(bh);
+	bp_account_hook(bh, BP_ACCOUNT_DIRTY);
 
 	if (journal->j_flags & JFS_BARRIER)
 		ret = __sync_dirty_buffer(bh, WRITE_SYNC | WRITE_FLUSH_FUA);
@@ -569,6 +570,7 @@
 			first_tag = 1;
 			set_buffer_jwrite(bh);
 			set_buffer_dirty(bh);
+			bp_account_hook(bh, BP_ACCOUNT_DIRTY);
 			wbuf[bufs++] = bh;
 
 			/* Record it so that we can wait for IO
diff -Naur linux-source-3.5.0/fs/jbd/revoke.c /remote/mnt/raid/workspace/linux-source-3.5.0/fs/jbd/revoke.c
--- linux-source-3.5.0/fs/jbd/revoke.c	2012-07-21 23:58:29.000000000 +0300
+++ /remote/mnt/raid/workspace/linux-source-3.5.0/fs/jbd/revoke.c	2012-12-10 14:30:26.000000000 +0200
@@ -651,6 +651,7 @@
 	set_buffer_jwrite(bh);
 	BUFFER_TRACE(bh, "write");
 	set_buffer_dirty(bh);
+	bp_account_hook(bh, BP_ACCOUNT_DIRTY);
 	write_dirty_buffer(bh, write_op);
 }
 #endif
diff -Naur linux-source-3.5.0/fs/jbd2/commit.c /remote/mnt/raid/workspace/linux-source-3.5.0/fs/jbd2/commit.c
--- linux-source-3.5.0/fs/jbd2/commit.c	2012-07-21 23:58:29.000000000 +0300
+++ /remote/mnt/raid/workspace/linux-source-3.5.0/fs/jbd2/commit.c	2012-12-10 14:30:20.000000000 +0200
@@ -624,6 +624,7 @@
 			first_tag = 1;
 			set_buffer_jwrite(bh);
 			set_buffer_dirty(bh);
+			bp_account_hook(bh, BP_ACCOUNT_DIRTY);
 			wbuf[bufs++] = bh;
 
 			/* Record it so that we can wait for IO
diff -Naur linux-source-3.5.0/fs/jbd2/journal.c /remote/mnt/raid/workspace/linux-source-3.5.0/fs/jbd2/journal.c
--- linux-source-3.5.0/fs/jbd2/journal.c	2012-11-13 19:49:36.000000000 +0200
+++ /remote/mnt/raid/workspace/linux-source-3.5.0/fs/jbd2/journal.c	2012-12-10 14:30:14.000000000 +0200
@@ -454,6 +454,7 @@
 	new_bh->b_blocknr = blocknr;
 	set_buffer_mapped(new_bh);
 	set_buffer_dirty(new_bh);
+	bp_account_hook(new_bh, BP_ACCOUNT_DIRTY);
 
 	*jh_out = new_jh;
 
diff -Naur linux-source-3.5.0/fs/jbd2/revoke.c /remote/mnt/raid/workspace/linux-source-3.5.0/fs/jbd2/revoke.c
--- linux-source-3.5.0/fs/jbd2/revoke.c	2012-07-21 23:58:29.000000000 +0300
+++ /remote/mnt/raid/workspace/linux-source-3.5.0/fs/jbd2/revoke.c	2012-12-10 14:30:08.000000000 +0200
@@ -680,6 +680,7 @@
 	set_buffer_jwrite(bh);
 	BUFFER_TRACE(bh, "write");
 	set_buffer_dirty(bh);
+	bp_account_hook(bh, BP_ACCOUNT_DIRTY);
 	write_dirty_buffer(bh, write_op);
 }
 #endif
diff -Naur linux-source-3.5.0/fs/mpage.c /remote/mnt/raid/workspace/linux-source-3.5.0/fs/mpage.c
--- linux-source-3.5.0/fs/mpage.c	2012-07-21 23:58:29.000000000 +0300
+++ /remote/mnt/raid/workspace/linux-source-3.5.0/fs/mpage.c	2012-12-15 12:27:34.620543734 +0200
@@ -372,6 +372,8 @@
 	struct buffer_head map_bh;
 	unsigned long first_logical_block = 0;
 
+	BP_SET_READ();
+
 	map_bh.b_state = 0;
 	map_bh.b_size = 0;
 	for (page_idx = 0; page_idx < nr_pages; page_idx++) {
@@ -392,6 +394,7 @@
 	BUG_ON(!list_empty(pages));
 	if (bio)
 		mpage_bio_submit(READ, bio);
+	BP_UNSET_READ();
 	return 0;
 }
 EXPORT_SYMBOL(mpage_readpages);
@@ -406,12 +409,15 @@
 	struct buffer_head map_bh;
 	unsigned long first_logical_block = 0;
 
+	BP_SET_READ();
+
 	map_bh.b_state = 0;
 	map_bh.b_size = 0;
 	bio = do_mpage_readpage(bio, page, 1, &last_block_in_bio,
 			&map_bh, &first_logical_block, get_block);
 	if (bio)
 		mpage_bio_submit(READ, bio);
+	BP_UNSET_READ();
 	return 0;
 }
 EXPORT_SYMBOL(mpage_readpage);
@@ -540,6 +546,7 @@
 		}
 		blocks[page_block++] = map_bh.b_blocknr;
 		boundary = buffer_boundary(&map_bh);
+		bp_account_page_buffer(page, &map_bh, BP_ACCOUNT_DIRTY);
 		bdev = map_bh.b_bdev;
 		if (block_in_file == last_block)
 			break;
diff -Naur linux-source-3.5.0/fs/readdir.c /remote/mnt/raid/workspace/linux-source-3.5.0/fs/readdir.c
--- linux-source-3.5.0/fs/readdir.c	2012-07-21 23:58:29.000000000 +0300
+++ /remote/mnt/raid/workspace/linux-source-3.5.0/fs/readdir.c	2012-12-08 19:55:13.000000000 +0200
@@ -17,6 +17,7 @@
 #include <linux/security.h>
 #include <linux/syscalls.h>
 #include <linux/unistd.h>
+#include <linux/buffer_head.h>
 
 #include <asm/uaccess.h>
 
@@ -37,7 +38,9 @@
 
 	res = -ENOENT;
 	if (!IS_DEADDIR(inode)) {
+		BP_SET_READ();
 		res = file->f_op->readdir(file, buf, filler);
+		BP_UNSET_READ();
 		file_accessed(file);
 	}
 	mutex_unlock(&inode->i_mutex);
diff -Naur linux-source-3.5.0/include/linux/buffer_head.h /remote/mnt/raid/workspace/linux-source-3.5.0/include/linux/buffer_head.h
--- linux-source-3.5.0/include/linux/buffer_head.h	2012-07-21 23:58:29.000000000 +0300
+++ /remote/mnt/raid/workspace/linux-source-3.5.0/include/linux/buffer_head.h	2012-12-15 12:27:48.252543865 +0200
@@ -347,4 +347,46 @@
 static inline int sync_mapping_buffers(struct address_space *mapping) { return 0; }
 
 #endif /* CONFIG_BLOCK */
+
+#if 0
+#define BP_SET_READ()	do {								\
+	if ((int)current->bp_flags < 0) {							\
+		if (printk_ratelimit())							\
+			printk(KERN_ERR "BP_SET_READ() - flags are negative! %s:%d\n", __FILE__, __LINE__);	\
+		current->bp_flags = 0;							\
+	} else {										\
+	if (current->bp_flags) {							\
+		if (printk_ratelimit())							\
+			printk(KERN_ERR "BP count already %d %s:%d", current->bp_flags, __FILE__, __LINE__);		\
+	}										\
+	current->bp_flags++;								\
+	}										\
+} while (0)
+#define BP_UNSET_READ()	do {								\
+	current->bp_flags--;								\
+	if ((int)current->bp_flags < 0) {							\
+		if (printk_ratelimit())							\
+			printk(KERN_ERR "BP_UNSET_READ() - flags are negative! %s:%d\n", __FILE__, __LINE__);	\
+		current->bp_flags = 0;							\
+	}										\
+} while (0)
+#define BP_IS_READ()	(current->bp_flags)
+#else
+#define BP_SET_READ()	do { current->bp_flags++; } while (0)
+#define BP_UNSET_READ()	do { current->bp_flags--; } while (0)
+#define BP_IS_READ()	(current->bp_flags)
+#endif
+
+#define BP_ACCOUNT_READ		0
+#define BP_ACCOUNT_GET		1
+#define BP_ACCOUNT_DIRTY	2
+#define BP_ACCOUNT_ACCESSED	3
+#define BP_ACCOUNT_INACTIVE	4
+#define BP_ACCOUNT_ACTIVATED	5
+#define BP_ACCOUNT_EVICTED	6
+void bp_account_hook(struct buffer_head *bh, int reason);
+void bp_account_page_locked(struct page *page, int reason);
+void bp_account_page_hook(struct page *page, int reason);
+void bp_account_page_buffer(struct page *page, struct buffer_head *bh, int reason);
+
 #endif /* _LINUX_BUFFER_HEAD_H */
diff -Naur linux-source-3.5.0/include/linux/sched.h /remote/mnt/raid/workspace/linux-source-3.5.0/include/linux/sched.h
--- linux-source-3.5.0/include/linux/sched.h	2012-11-13 19:49:36.000000000 +0200
+++ /remote/mnt/raid/workspace/linux-source-3.5.0/include/linux/sched.h	2012-12-01 17:32:07.000000000 +0200
@@ -1417,6 +1417,8 @@
 #endif
 	struct seccomp seccomp;
 
+	u32 bp_flags;
+
 /* Thread group tracking */
    	u32 parent_exec_id;
    	u32 self_exec_id;
diff -Naur linux-source-3.5.0/mm/filemap.c /remote/mnt/raid/workspace/linux-source-3.5.0/mm/filemap.c
--- linux-source-3.5.0/mm/filemap.c	2012-11-13 19:49:36.000000000 +0200
+++ /remote/mnt/raid/workspace/linux-source-3.5.0/mm/filemap.c	2012-12-12 14:13:18.654105994 +0200
@@ -162,6 +162,7 @@
 
 	freepage = mapping->a_ops->freepage;
 	spin_lock_irq(&mapping->tree_lock);
+	bp_account_page_locked(page, BP_ACCOUNT_EVICTED);
 	__delete_from_page_cache(page);
 	spin_unlock_irq(&mapping->tree_lock);
 	mem_cgroup_uncharge_cache_page(page);
@@ -409,6 +410,7 @@
 		new->index = offset;
 
 		spin_lock_irq(&mapping->tree_lock);
+		bp_account_page_locked(old, BP_ACCOUNT_EVICTED);
 		__delete_from_page_cache(old);
 		error = radix_tree_insert(&mapping->page_tree, offset, new);
 		BUG_ON(error);
diff -Naur linux-source-3.5.0/mm/page-writeback.c /remote/mnt/raid/workspace/linux-source-3.5.0/mm/page-writeback.c
--- linux-source-3.5.0/mm/page-writeback.c	2012-07-21 23:58:29.000000000 +0300
+++ /remote/mnt/raid/workspace/linux-source-3.5.0/mm/page-writeback.c	2012-12-12 14:16:58.130108109 +0200
@@ -1988,6 +1989,7 @@
 		if (mapping2) { /* Race with truncate? */
 			BUG_ON(mapping2 != mapping);
 			WARN_ON_ONCE(!PagePrivate(page) && !PageUptodate(page));
+			bp_account_page_locked(page, BP_ACCOUNT_DIRTY);
 			account_page_dirtied(page, mapping);
 			radix_tree_tag_set(&mapping->page_tree,
 				page_index(page), PAGECACHE_TAG_DIRTY);
@@ -2063,6 +2065,7 @@
 		 */
 		ClearPageReclaim(page);
 #ifdef CONFIG_BLOCK
+		bp_account_page_locked(page, BP_ACCOUNT_DIRTY);
 		if (!spd)
 			spd = __set_page_dirty_buffers;
 #endif
diff -Naur linux-source-3.5.0/mm/rmap.c /remote/mnt/raid/workspace/linux-source-3.5.0/mm/rmap.c
--- linux-source-3.5.0/mm/rmap.c	2012-07-21 23:58:29.000000000 +0300
+++ /remote/mnt/raid/workspace/linux-source-3.5.0/mm/rmap.c	2012-12-12 14:15:52.306107476 +0200
@@ -56,6 +56,7 @@
 #include <linux/mmu_notifier.h>
 #include <linux/migrate.h>
 #include <linux/hugetlb.h>
+#include <linux/buffer_head.h> /* bp_account_page */
 
 #include <asm/tlbflush.h>
 
@@ -1145,6 +1147,7 @@
 	bool locked;
 	unsigned long flags;
 
+	bp_account_page_hook(page, BP_ACCOUNT_ACTIVATED);
 	mem_cgroup_begin_update_page_stat(page, &locked, &flags);
 	if (atomic_inc_and_test(&page->_mapcount)) {
 		__inc_zone_page_state(page, NR_FILE_MAPPED);
@@ -1173,6 +1176,8 @@
 	if (!anon)
 		mem_cgroup_begin_update_page_stat(page, &locked, &flags);
 
+	bp_account_page_hook(page, BP_ACCOUNT_INACTIVE);
+
 	/* page still mapped by someone else? */
 	if (!atomic_add_negative(-1, &page->_mapcount))
 		goto out;
diff -Naur linux-source-3.5.0/mm/swap.c /remote/mnt/raid/workspace/linux-source-3.5.0/mm/swap.c
--- linux-source-3.5.0/mm/swap.c	2012-07-21 23:58:29.000000000 +0300
+++ /remote/mnt/raid/workspace/linux-source-3.5.0/mm/swap.c	2012-12-12 14:18:35.798109052 +0200
@@ -30,6 +30,7 @@
 #include <linux/backing-dev.h>
 #include <linux/memcontrol.h>
 #include <linux/gfp.h>
+#include <linux/buffer_head.h>
 
 #include "internal.h"
 
@@ -353,6 +354,7 @@
 	if (PageLRU(page) && !PageActive(page) && !PageUnevictable(page)) {
 		struct pagevec *pvec = &get_cpu_var(activate_page_pvecs);
 
+		bp_account_page_hook(page, BP_ACCOUNT_ACTIVATED);
 		page_cache_get(page);
 		if (!pagevec_add(pvec, page))
 			pagevec_lru_move_fn(pvec, __activate_page, NULL);
@@ -369,6 +371,7 @@
 {
 	struct zone *zone = page_zone(page);
 
+	bp_account_page_hook(page, BP_ACCOUNT_ACTIVATED);
 	spin_lock_irq(&zone->lru_lock);
 	__activate_page(page, mem_cgroup_page_lruvec(page, zone), NULL);
 	spin_unlock_irq(&zone->lru_lock);
@@ -390,6 +393,7 @@
 		ClearPageReferenced(page);
 	} else if (!PageReferenced(page)) {
 		SetPageReferenced(page);
+		bp_account_page_hook(page, BP_ACCOUNT_ACCESSED);
 	}
 }
 EXPORT_SYMBOL(mark_page_accessed);
@@ -488,6 +492,8 @@
 	file = page_is_file_cache(page);
 	lru = page_lru_base_type(page);
 
+	bp_account_page_hook(page, BP_ACCOUNT_INACTIVE);
+
 	del_page_from_lru_list(page, lruvec, lru + active);
 	ClearPageActive(page);
 	ClearPageReferenced(page);
diff -Naur linux-source-3.5.0/mm/vmscan.c /remote/mnt/raid/workspace/linux-source-3.5.0/mm/vmscan.c
--- linux-source-3.5.0/mm/vmscan.c	2012-11-13 19:49:36.000000000 +0200
+++ /remote/mnt/raid/workspace/linux-source-3.5.0/mm/vmscan.c	2012-12-12 14:19:26.090109537 +0200
@@ -498,6 +498,8 @@
 
 		freepage = mapping->a_ops->freepage;
 
+		bp_account_page_locked(page, BP_ACCOUNT_EVICTED);
+
 		__delete_from_page_cache(page);
 		spin_unlock_irq(&mapping->tree_lock);
 		mem_cgroup_uncharge_cache_page(page);
